{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CfOgI8GHWIG1"
   },
   "source": [
    "Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ogh-USrxc6IR"
   },
   "source": [
    "Perpceptron Class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "k7fipBPaA_H-"
   },
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "  # Initializing weights,learning rate aand epochs\n",
    "  def __init__(self,eta,epochs):\n",
    "    # Random Weight Assignment\n",
    "    self.weights = np.random.randn(3) * 1e-4\n",
    "    print(\"Initial weights\",self.weights)\n",
    "    self.eta = eta\n",
    "    self.epochs = epochs\n",
    "\n",
    "\n",
    "# Defining the Activation Function - using the step function\n",
    "  def activationFunction(self,inputs,weights):\n",
    "    # Multiplying weights and the input values \n",
    "    z = np.dot(inputs,weights)\n",
    "    return np.where(z>0,1,0)\n",
    "\n",
    "# Defining the Fit Method\n",
    "  def fit(self,X,y):\n",
    "    self.X=X\n",
    "    self.y=y\n",
    "    # Adding bias to the input values \n",
    "    X_with_bias = np.c_[self.X,-np.ones((len(self.X),1))]\n",
    "    print(\"X with Bias \",X_with_bias)\n",
    "\n",
    "    # Forward  & backward Propogation\n",
    "    for epoch in range(self.epochs):\n",
    "          print(\"====================\")\n",
    "          print(\"for epoch\",epoch)\n",
    "          print(\"====================\")\n",
    "\n",
    "          y_hat = self.activationFunction(X_with_bias,self.weights)\n",
    "          print(\"predicted values\",y_hat)\n",
    "          self.error = self.y - y_hat\n",
    "          # Weights updation \n",
    "          self.weights = self.weights + self.eta* np.dot(X_with_bias.T,self.error)\n",
    "          print(\"Updated weights after epoch\",self.epochs,self.weights)\n",
    "\n",
    "\n",
    "# Defining the Predict Method\n",
    "  def predict(self,X):\n",
    "    X_with_bias = np.c_[self.X,-np.ones((len(self.X),1))]\n",
    "    return self.activationFunction(X_with_bias,self.weights)\n",
    "\n",
    "# Defining a method to calculate the loss\n",
    "  def total_loss(self):\n",
    "    total_loss = np.sum(self.error)\n",
    "    print(\"Toal loss\",total_loss)\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "tqp-6mWLEe_g"
   },
   "outputs": [],
   "source": [
    "# A Method to separate the independent and Dependent features\n",
    "def prepare_data(df):\n",
    "    X=df.drop(\"y\",axis=1)\n",
    "    y=df['y']\n",
    "  \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "G6nJWuTIEwV5"
   },
   "outputs": [],
   "source": [
    "# Populating Data to implement the AND Logic gate\n",
    "AND ={\n",
    "    \n",
    "    \"x1\":[0,0,1,1],\n",
    "    \"x2\":[0,1,0,1],\n",
    "    \"y\":[0,0,0,1]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(AND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Tkxu_BkNE-je"
   },
   "outputs": [],
   "source": [
    "X,y = prepare_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Perceptron & Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YoM7wslBFDB3",
    "outputId": "d38dca36-8593-46fa-b8c6-5601ac67d699"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights [1.17572960e-04 1.07318631e-04 7.91509791e-05]\n",
      "X with Bias  [[ 0.  0. -1.]\n",
      " [ 0.  1. -1.]\n",
      " [ 1.  0. -1.]\n",
      " [ 1.  1. -1.]]\n",
      "====================\n",
      "for epoch 0\n",
      "====================\n",
      "predicted values [0 1 1 1]\n",
      "Updated weights after epoch 10 [-0.29988243 -0.29989268  0.60007915]\n",
      "====================\n",
      "for epoch 1\n",
      "====================\n",
      "predicted values [0 0 0 0]\n",
      "Updated weights after epoch 10 [1.17572960e-04 1.07318631e-04 3.00079151e-01]\n",
      "====================\n",
      "for epoch 2\n",
      "====================\n",
      "predicted values [0 0 0 0]\n",
      "Updated weights after epoch 10 [3.00117573e-01 3.00107319e-01 7.91509791e-05]\n",
      "====================\n",
      "for epoch 3\n",
      "====================\n",
      "predicted values [0 1 1 1]\n",
      "Updated weights after epoch 10 [1.17572960e-04 1.07318631e-04 6.00079151e-01]\n",
      "====================\n",
      "for epoch 4\n",
      "====================\n",
      "predicted values [0 0 0 0]\n",
      "Updated weights after epoch 10 [0.30011757 0.30010732 0.30007915]\n",
      "====================\n",
      "for epoch 5\n",
      "====================\n",
      "predicted values [0 1 1 1]\n",
      "Updated weights after epoch 10 [1.17572960e-04 1.07318631e-04 9.00079151e-01]\n",
      "====================\n",
      "for epoch 6\n",
      "====================\n",
      "predicted values [0 0 0 0]\n",
      "Updated weights after epoch 10 [0.30011757 0.30010732 0.60007915]\n",
      "====================\n",
      "for epoch 7\n",
      "====================\n",
      "predicted values [0 0 0 1]\n",
      "Updated weights after epoch 10 [0.30011757 0.30010732 0.60007915]\n",
      "====================\n",
      "for epoch 8\n",
      "====================\n",
      "predicted values [0 0 0 1]\n",
      "Updated weights after epoch 10 [0.30011757 0.30010732 0.60007915]\n",
      "====================\n",
      "for epoch 9\n",
      "====================\n",
      "predicted values [0 0 0 1]\n",
      "Updated weights after epoch 10 [0.30011757 0.30010732 0.60007915]\n",
      "Toal loss 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ETA=0.3  # Learning Rate\n",
    "EPOCHS = 10 # Epochs\n",
    "\n",
    "model = Perceptron(eta=ETA, epochs=EPOCHS)\n",
    "model.fit(X,y)\n",
    "model.total_loss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Perceptron Algorithm.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
